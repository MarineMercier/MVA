{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP-super-resolution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJChli2fHjPK"
      },
      "source": [
        "# Super resolution\n",
        "\n",
        "The objective of this practical session is to create a multi-frame super-resolution algorithm and apply it to real satellite images. \n",
        "\n",
        "\n",
        "We cover the following topics:\n",
        "* Image interpolation and sampling\n",
        "* Simulate realistic series of low-resolution frames\n",
        "* Fuse the samples from a set of registered low-resolution images into a high resolution one\n",
        "* Register a set of frames\n",
        "* Application to real satellite images \n",
        "\n",
        "\n",
        "### Name: Mercier Marine \n",
        "\n",
        "#### Instructions\n",
        "To solve this practical session, answer the questions below. Then, clear all the output cells using the menu option **Cell->All Output->Clear** and export the notebook with your answers using the menu option **File -> Download as -> Notebook (.ipynb)**. Then [submit the resulting file here](https://forms.gle/k6FSX2BrPix823dx6) by next week. You will receive an automatic acknowledgement of receipt.\n",
        "\n",
        "This TP contains 6 exercices that are mainly focused on studying the properties of the presented algorithmic bricks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8lv6l2kHjPN",
        "outputId": "835896ad-edb1-4b77-a2bf-c59cbb73c9b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Setup code for the notebook\n",
        "\n",
        "# The following lines install the necessary packages in the colab environment\n",
        "try:\n",
        "    from google.colab import files\n",
        "\n",
        "    # download TP data and tools\n",
        "    !wget -q http://boucantrin.ovh.hw.ipol.im/static/facciolo/mvaisat/tp6.zip\n",
        "    !unzip -q -o tp6.zip\n",
        "    \n",
        "    # install dependencies\n",
        "    !apt-get -qq install libnfft3-dev\n",
        "    !python -m pip -q install numpy matplotlib scipy geojson pyproj==2.4.1 opencv-contrib-python==4.5.3.56 rasterio  numba  pyfftw pyvoronoi rpcm\n",
        "    !python -m pip -q install git+https://github.com/pyNFFT/pyNFFT.git@e2da0af374c6d7cc38992936ce4dda6e6c0ad7f1\n",
        "\n",
        "except ImportError:\n",
        "#    %matplotlib notebook\n",
        "    pass\n",
        "\n",
        "# Autoreload python modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np                   # numeric linear algebra\n",
        "import scipy\n",
        "import scipy.ndimage                # only for ndimage.affine_transform\n",
        "import matplotlib.pyplot as plt      # plotting\n",
        "from numpy.fft import fft, ifft, fft2, ifft2, fftshift, ifftshift, fftfreq\n",
        "from nfft_tools import initS, applyS, applySadj\n",
        "\n",
        "# imports specific to this course\n",
        "import utils          # IO and conversion tools (from TP-collection)\n",
        "import vistools       # display tools (from TP-collection)\n",
        "from sr import imshowfft, fft_translate, fft_zoom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libfftw3-long3:amd64.\n",
            "(Reading database ... 155113 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfftw3-long3_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-long3:amd64 (3.3.7-1) ...\n",
            "Selecting previously unselected package libfftw3-quad3:amd64.\n",
            "Preparing to unpack .../1-libfftw3-quad3_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-quad3:amd64 (3.3.7-1) ...\n",
            "Selecting previously unselected package libfftw3-single3:amd64.\n",
            "Preparing to unpack .../2-libfftw3-single3_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-single3:amd64 (3.3.7-1) ...\n",
            "Selecting previously unselected package libfftw3-bin.\n",
            "Preparing to unpack .../3-libfftw3-bin_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-bin (3.3.7-1) ...\n",
            "Selecting previously unselected package libfftw3-dev:amd64.\n",
            "Preparing to unpack .../4-libfftw3-dev_3.3.7-1_amd64.deb ...\n",
            "Unpacking libfftw3-dev:amd64 (3.3.7-1) ...\n",
            "Selecting previously unselected package libnfft3-double2:amd64.\n",
            "Preparing to unpack .../5-libnfft3-double2_3.3.2-2_amd64.deb ...\n",
            "Unpacking libnfft3-double2:amd64 (3.3.2-2) ...\n",
            "Selecting previously unselected package libnfft3-long2:amd64.\n",
            "Preparing to unpack .../6-libnfft3-long2_3.3.2-2_amd64.deb ...\n",
            "Unpacking libnfft3-long2:amd64 (3.3.2-2) ...\n",
            "Selecting previously unselected package libnfft3-single2:amd64.\n",
            "Preparing to unpack .../7-libnfft3-single2_3.3.2-2_amd64.deb ...\n",
            "Unpacking libnfft3-single2:amd64 (3.3.2-2) ...\n",
            "Selecting previously unselected package libnfft3-dev:amd64.\n",
            "Preparing to unpack .../8-libnfft3-dev_3.3.2-2_amd64.deb ...\n",
            "Unpacking libnfft3-dev:amd64 (3.3.2-2) ...\n",
            "Setting up libfftw3-quad3:amd64 (3.3.7-1) ...\n",
            "Setting up libnfft3-double2:amd64 (3.3.2-2) ...\n",
            "Setting up libfftw3-single3:amd64 (3.3.7-1) ...\n",
            "Setting up libfftw3-long3:amd64 (3.3.7-1) ...\n",
            "Setting up libfftw3-bin (3.3.7-1) ...\n",
            "Setting up libnfft3-single2:amd64 (3.3.2-2) ...\n",
            "Setting up libfftw3-dev:amd64 (3.3.7-1) ...\n",
            "Setting up libnfft3-long2:amd64 (3.3.2-2) ...\n",
            "Setting up libnfft3-dev:amd64 (3.3.2-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 24.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56.1 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 19.3 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 63.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 21.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 46.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 51.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 47.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 71.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 71.1 MB/s \n",
            "\u001b[?25h  Building wheel for pyvoronoi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for srtm4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ahaqwOpHjPO"
      },
      "source": [
        "# I. Image model, sampling and interpolation\n",
        "\n",
        "Shannon's sampling theorem states that a band limited image $u$ can be **reconstructed exactly from its integer samples** via sinc interpolation\n",
        "$$u(x) = \\sum_{k\\in\\mathbb{Z}^2} u(k) \\operatorname{sinc}(x-k).$$\n",
        "\n",
        "For finite images of size $N^2$ the above formula leads to a trigonometric polynomial interpolation model \n",
        "$$u(x) = \\sum_{\\omega \\in [-N/2...N/2-1]^2} \\hat u(\\omega) \\exp ( -i 2\\pi \\omega x/N  ),$$\n",
        "where  the coefficients $\\hat u(\\omega) $ are obtained as the discrete fourier transform (DFT) of the integer samples of $u$.\n",
        "\n",
        "\n",
        "However, this interpolation method is not practical for numerical purposes because the sinc kernel is infinitely supported and the evaluation of the (finite) trigonometric polynomial involves all the $N^2$ samples of $\\hat u(\\omega)$. We can obtain an approximation by truncating this kernel, but we still need a very large kernel for good accuracy.\n",
        "\n",
        "There are two alternatives to obtain good accuracy:\n",
        "1. Use properties of the Fourier transform to obtain exact interpolations in particular cases\n",
        "1. Use compactly supported kernels that are good approximations of the sinc kernel combined with exact interpolation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIQKkS1eHjPP"
      },
      "source": [
        "# the input image u\n",
        "u = utils.readGTIFF('data/barb.png').squeeze()[100:356,100:356]\n",
        "sh = u.shape\n",
        "# show the image and its fourier transform\n",
        "imshowfft( u )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpi-BSXkHjPP"
      },
      "source": [
        "### Fourier interpolation\n",
        "\n",
        "Assume we want to obtain the interpolated values $u(x)$ for $x$ on a grid $\\Gamma = T(\\mathbb{Z}^2)$ which is a transformation of the original sampling grid. When $T$ has a particular structure we can use some properties of the Fourier transform to compute the exact interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUl7OHBlHjPQ"
      },
      "source": [
        "#### Case 1:  $T$ is a translation\n",
        "\n",
        "If $T(x) = x+t$ for some $t\\in\\mathbb{R}^2$ then we can use the following property of the Fourier transform\n",
        "\n",
        "$ \\mathcal{F}(u \\circ T)(\\omega) = e^{2\\pi i \\omega t} \\mathcal{F}(u)(\\omega) $\n",
        "\n",
        "The function `fft_translate` uses this property and the FFT algorithm to translate an image by a given amount."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYaiPajAHjPQ"
      },
      "source": [
        "def fft_translate_v0(im, dx, dy):\n",
        "    '''\n",
        "    Subpixel translation using Fourier interpolation\n",
        "    \n",
        "    Args:\n",
        "        im (np.ndarray): numpy array containing the input image\n",
        "        dx,dy (floats): subpixel shift to be applied\n",
        "    Returns:\n",
        "        np.ndarray: containing the resampled shifted image \n",
        "    '''\n",
        "    import pyfftw\n",
        "    import numpy as np\n",
        "\n",
        "    imF = np.fft.fft2(im)\n",
        "    X, Y = np.meshgrid(np.fft.fftfreq(im.shape[1]), np.fft.fftfreq(im.shape[0]))\n",
        "    phi =  2j*np.pi*( X*dx + Y*dy ) \n",
        "    imFshift = imF * np.exp(-phi)\n",
        "    return  np.real(np.fft.ifft2(imFshift))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE9R6iQ9HjPQ"
      },
      "source": [
        "# here we test the translation on an image \n",
        "dx,dy = 10.3, 8.8\n",
        "ut = fft_translate_v0(u, dx,dy)\n",
        "\n",
        "vistools.display_gallery([ut, u],['translated by: %g,%g'%(dx,dy), 'original'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ2YWMtcHjPR"
      },
      "source": [
        "Note that the image is correctly translated, however the periodicity assumption of the DFT leads to ringing effects at the image boundaries. \n",
        "\n",
        "Nevertheless, the representation is exact and we should be able to get back to the initial image. \n",
        "\n",
        "> **<u>Excercise 1</u>**: Check that `fft_translate` works correctly. Choose two **NON INTEGER** translations $t_1$ and $t_2$ such that $t_1 + t_2 = 0$. Translate an image by $t_1$ then by $t_2$. Compare the result to image $u$ and check the error with respect to the translated image.\n",
        ">\n",
        "> **<u>Bonus</u>**: We could keep translating the image by non-integer steps back and forth without ever loosing precision. Try doing 100 translations with `fft_translate` and compare with bilinear or bicubinc interoplation (which are implemented in `scipy.ndimage.map_coordinates`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gqpess0HjPR"
      },
      "source": [
        "### EXERCISE 1. WRITE YOUR SOLUTION CODE HERE*\n",
        "from scipy import ndimage\n",
        "t1 = [3.8, 2.7] \n",
        "ut = u.copy()\n",
        "vt = u.copy()\n",
        "\n",
        "for i in range(100):\n",
        "    ut = fft_translate_v0(ut, t1[0], t1[1])\n",
        "    ut = fft_translate_v0(ut, -t1[0], -t1[1])\n",
        "\n",
        "    vt = ndimage.interpolation.map_coordinates(vt, mode='nearest')\n",
        "\n",
        "vistools.display_gallery([ut, u],['result images', 'original'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgZKe80nHjPS"
      },
      "source": [
        "In practice the rebounds resulting from the periodization are a nuance, and there are different ways to suppress them.\n",
        "\n",
        "- One consists in symmetrizing the image before applying the transformation (which is implemented in `sr.fft_translate`). \n",
        "\n",
        "- Another option is to smooth the image close to the image boundaries so that there are no discontinuities in the periodic image (see [Moisan 2011](http://helios.mi.parisdescartes.fr/~moisan/papers/download.php?file=2009-11r.pdf) for instance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGJi-GQHHjPS"
      },
      "source": [
        "dx,dy = 10.3, 8.8\n",
        "ut = fft_translate(u, dx,dy)\n",
        "\n",
        "vistools.display_gallery([u,ut],['original','translated by: %g,%g'%(dx,dy)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD66rhXHHjPT"
      },
      "source": [
        "#### Case 2: $T$ is a zoom\n",
        "\n",
        "If $T(x) = x/z$ for some zoom factor $z\\in\\mathbb{Z}$ then we can obtain a zoomed version of $u$ by zero padding.  The  function `fft_zoom` uses this property and the FFT algorithm to obtain a zoomed version of the input image.\n",
        "\n",
        "> **<u>Excercise 2</u>**: Check that `fft_zoom` works correctly. You will perform two checks:\n",
        ">\n",
        "> 1. Compute a x2 zoom and check that the even pixels of the zoomed image coincide with the pixels of the original image\n",
        "> 2. Visualize the spectrum of the zoomed-in image in log scale (only the central part of the spectrum should be non-zero). You can use `imshowfft` function for this visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K-RD_kBHjPT"
      },
      "source": [
        "def fft_zoom(image, z=2):\n",
        "    \"\"\"\n",
        "    Zoom an image by zero-padding its Discrete Fourier transform.\n",
        "    \n",
        "    Args:\n",
        "        image (np.ndarray): 2D grid of pixel values.\n",
        "        z (int): Factor by which to multiply the dimensions of the image. Must be >= 1.\n",
        "    Returns:\n",
        "        np.ndarray: zoomed image.\n",
        "    \"\"\"\n",
        "    h, w = image.shape\n",
        "\n",
        "    # zero padding sizes\n",
        "    padw = (z - 1) * w\n",
        "    padh = (z - 1) * h\n",
        "    \n",
        "    # Fourier transform with the zero-frequency at 0,0\n",
        "    ft = np.fft.fft2(image)\n",
        "\n",
        "    # the zoom-in is performed by zero padding the Fourier transform\n",
        "    ft = np.hstack((ft[:,:w//2], np.zeros((h,padw))     , ft[:,w//2:]))    \n",
        "    ft = np.vstack((ft[:h//2,:], np.zeros((padh,w+padw)), ft[h//2:,:]))    \n",
        "\n",
        "    # take the inverse Fourier transform\n",
        "    out = np.fft.ifft2(ft)\n",
        "\n",
        "    # if the input is a real-valued image, then keep only the real part\n",
        "    if np.isrealobj(image):\n",
        "        out = np.real(out)\n",
        "\n",
        "    # to preserve the values of the original samples, the L2 norm has to be\n",
        "    # multiplied by z*z.\n",
        "    return out * z * z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dySXdM0HjPT"
      },
      "source": [
        "### EXERCISE 2. WRITE YOUR SOLUTION CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwS2mHN1HjPT"
      },
      "source": [
        "###   B-spline + FFT interpolation = NFFT\n",
        "\n",
        "A common alternative to Fourier-based interpolation is the family of B-spline interpolators of different orders.  When the order of the spline tends to infinity spline interpolation converges to sinc interpolation. However increasing the spline order also increases the computational cost because the support of the kernel is proportional to the spline order (squared in dimension 2).\n",
        "\n",
        "A better computation/accuracy tradeoff can be obtained by combining a zoom by fft up to a fixed zoom level and B-spline interpolation of a moderate order on the zoomed image.\n",
        "\n",
        "\n",
        "**The nonuniform FFT library (NFFT)** uses these techniques to implement efficient and precise trigonometric polynomial evaluation functions. \n",
        "\n",
        "Conceptually, NFFT can be seen as implementing a linear operator $S$ of size $M\\times N$, where $N$ is the number of coefficients in the trigonometric polynomial, and $M$ is the number of irregularly sampled the pixels at positions $\\xi_i$\n",
        "$$((S_{ij}))  =  exp( -i 2\\pi \\xi_i j/N).$$\n",
        "\n",
        "Applying $S$ to a Fourier transform $\\hat u$ evaluates the trigonomatric polynomial on the samples:  $u(\\xi_i) = \\sum_j  S_{ij}  \\hat u_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxj9oxlcHjPU"
      },
      "source": [
        "There are many nuances in the NFFT, which are incapsulated in the module `nfft_tools`. The two main functions exposed by this module are `initS` and `applyS` which define and apply the sampling operator $S$. \n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "    def initS(samples, N, nfft_m=9):\n",
        "        \"\"\"\n",
        "        prepare the NFFT plan for the samples\n",
        "        Args:\n",
        "            samples: array of size (M,2) with the 2D coordinates (in the interval [0,1]^2) of the M samples\n",
        "            N      : is a 2-element array specifying the 2D bandwidth or the order of the trigonometric polynomial\n",
        "            nfft_m : is an internal parameter controlling the quality of the sampling \n",
        "        Returns:\n",
        "            the NFFT plan needed to apply the transformation on a data vector\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "    def applyS(fhat, p):\n",
        "        \"\"\"\n",
        "        Samples the trigonometric polynomial fhat using NFFT\n",
        "        Args:\n",
        "            fhat: is the regular 2D fft of an image **(but fftshifted and conjugated)**, \n",
        "            p   : is the plan specifying the positions of the samples to be computed \n",
        "        Returns:\n",
        "            the values sampled at the positions indicated in the plan\n",
        "        \"\"\"\n",
        "\n",
        " ```\n",
        "\n",
        "The following blocks use these functions first to verify the consistency of NFFT and FFT, and then to sample the trigonometric polynomial associated to an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtI7TmEbHjPU"
      },
      "source": [
        "#### Sanity check: sampling the trigonometric polynomial at the regular positions yields the same image as the IFFT\n",
        "\n",
        "Since the NFFT S operator is computing an approximate IFFT it should coincide with it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooiidXP1HjPU"
      },
      "source": [
        "from numpy.fft import fftshift, ifftshift, fft2, ifft2\n",
        "from nfft_tools import initS, applyS, applySadj, initT, applyT\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "u = utils.readGTIFF('data/barb.png')[:,:,0].squeeze()#.astype(np.double)[:250,:257]\n",
        "\n",
        "imshowfft(u)\n",
        "\n",
        "\n",
        "# shifted conjugated fft is the representation of nfft \n",
        "fft_u = fft2(u) \n",
        "fft_u2 = fftshift(fft_u).conj()   \n",
        "\n",
        "\n",
        "# regular grid size \n",
        "N = u.shape\n",
        "\n",
        "# irregular sample  positions\n",
        "xx, yy = np.meshgrid( np.arange(N[1], dtype=np.double)/(N[1]) ,  np.arange(N[0], dtype=np.double)/(N[0]) )\n",
        "\n",
        "samples = np.array(list (zip(yy.flatten(), xx.flatten()))  )\n",
        "M = len( xx.flatten() )   # number of samples \n",
        "\n",
        "## output: xx, yy, samples, M, shape\n",
        "\n",
        "print(N,M)\n",
        "\n",
        "# initialize the nfft with this samplig grid\n",
        "p = initS(samples, N)\n",
        "\n",
        "# compute the inverse fft using nfft\n",
        "irreg = applyS(fft_u2,p)\n",
        "u2 = np.real(irreg.reshape((N[0],N[1])) )\n",
        "\n",
        "# check the difference of the reconstruction \n",
        "plt.figure()\n",
        "plt.imshow( u - u2, vmin=-1,vmax=1 )\n",
        "plt.title('difference')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvkN3O3EHjPV"
      },
      "source": [
        "#### Sample an image on an irregular grid (here perturbed)\n",
        "\n",
        "Here we show how NFFT allows to efficiently sample the trigonometric polynomial of an image without restrictions on the shape of the sampling grid. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TcM7PEOHjPV"
      },
      "source": [
        "# regular grid size \n",
        "N = u.shape\n",
        "\n",
        "# irregular sample positions: a smoothly perturbed grid\n",
        "xx, yy = np.meshgrid( np.arange(N[1], dtype=np.double)/(N[1]) ,  np.arange(N[0], dtype=np.double)/(N[0]) )\n",
        "xx =  xx + 1*scipy.ndimage.gaussian_filter(np.random.randn(*xx.shape)/4, 8)\n",
        "yy =  yy + 1*scipy.ndimage.gaussian_filter(np.random.randn(*yy.shape)/4, 8)\n",
        "\n",
        "samples = np.array(list (zip(yy.flatten(), xx.flatten()))  )\n",
        "M = len( xx.flatten() )   # number of samples \n",
        "\n",
        "fft_u2 = fftshift(fft2(u)).conj()   # shifted conjugated fft is the representation of nfft \n",
        "\n",
        "# init nfft\n",
        "p = initS(samples, N)\n",
        "\n",
        "irreg = applyS(fft_u2,p)\n",
        "\n",
        "# show the irregular samples\n",
        "imshowfft(  np.real(irreg.reshape((N[0],N[1])) ) , title='image sampled on a perturbed grid' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UeKvCvnHjPV"
      },
      "source": [
        "# II. Image simulation\n",
        "\n",
        "In this section we will simulate low resolution images as aquired by the Skysat satellite. \n",
        "We consider the following simplified image formation model \n",
        "\n",
        "$$ v =  \\Delta   ( T  ( K_{\\text{diffraction}} \\ast K_{\\text{sensor}}\\ast u )  ) + \\text{noise},$$ \n",
        "\n",
        "where $u$ is an ideal image (of infinite resolution), $K_{\\text{diffraction}}$ is the PSF due to the diffraction,   $K_{\\text{sensor}}$ the PSF due to the sensor integration, $T$ denotes a spatial transformation, and $\\Delta$ denotes the sampling step. Note that in this formulation the spatial transformation $T$ was commuted with convolutions and in general this is not correct. However, for the case of remote sensing the transformation $T$ is almost a rigid transformation in  which case the commutation is correct.  \n",
        "\n",
        "Since convolutions are easily modeled in the frequency domain we instead manipulate the OTF (optical transfer functions) associated to our kernels: $H_{\\text{diffraction}}$, and $H_{\\text{sensor}}$.\n",
        "\n",
        "In order to correctly represent the alias resulting from the sampling we shall simulate our images at an oversampled rate controlled by the zoom factor $z=2$. This \"zoomed\" image will be our infinite resolution image. For simplicity, our input image will be squared and of size $N\\times N$. \n",
        "It is important to note that the pixels of this oversampled image are therefore of size $3.25µm \\times 3.25µm$: half the size of the target resolution. \n",
        "\n",
        "These are the relevant parameters to be considered (in cm)\n",
        "\n",
        "    D = 35               #cm             # circular aperture diameter \n",
        "    f = 360              #cm = 3.6m      # focal lenght \n",
        "    lam = 6e-5           #cm = 0.6µm     # visible light wavelenght (green)\n",
        "    pixel_pitch = 6.5e-4 #cm = 6.5µm     # spacing of the pixels (pixel area is slightly smaller)\n",
        "    alt = 5e7            #cm = 500km     # altitude of the satellite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSqUYEh7HjPV"
      },
      "source": [
        "# skysat's parameters \n",
        "\n",
        "D=35      #cm \n",
        "f=360     #cm = 3.6 m\n",
        "lam=6e-5  #cm = 0.6 micrometers   # ~visible light \n",
        "pixel_pitch = 6.5e-4 #cm   6.5 µm\n",
        "photodiode_ratio=0.8 #assumed relative size of the photodiode wrt the pixel_pitch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2e5_Nf7HjPW"
      },
      "source": [
        "### Sensor integration OTF\n",
        "\n",
        "<img width=500px src=\"https://d1d1c1tnh6i0t6.cloudfront.net/wp-content/uploads/2017/08/pixel-size-2.jpg\">\n",
        "\n",
        "The OTF associated to the sensor integration is defined relative to the sampling step.\n",
        "Assuming a squared integration window the 2D OTF is the sinus cardinal $sinc(\\omega_x/2)sinc(\\omega_y/2)$.\n",
        "\n",
        "Given that the photodiode area is just a part of the surface of the pixel we should consider an integration window that is smaller than the sampling step, this amounts to changing the width of the $sinc$. \n",
        "\n",
        "Let us start by computing and displaying the $sinc$ profile, zoomed in order to see its effects on the higher frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqQj849iHjPW"
      },
      "source": [
        "# the input image u\n",
        "u = utils.readGTIFF('data/barb.png').squeeze()[100:356,100:356]\n",
        "sh = u.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG5_72sqHjPW"
      },
      "source": [
        "def sinc_OTF_profile(N, zoom, photodiode_ratio=1):\n",
        "    '''\n",
        "    Generates the zoomed version of the 1D sensor OTF profile\n",
        "    Args:\n",
        "        N: is the lenght of the signal to be produced\n",
        "        zoom: specifies the oversampling factor. zoom=2 yields the OTF at 2 x Nyquist\n",
        "        photodiode_ratio (default 1): is the size of the pixel relative to the separation\n",
        "    Returns:\n",
        "        a vector w with the sampled frequencies \n",
        "        and a vector with the corresponding OTF values\n",
        "    '''\n",
        "    # frequency range [-z*pi, z*pi]\n",
        "    #w = np.arange(-N/2,N/2)*zoom*np.pi*2/N\n",
        "    w = fftshift(fftfreq(N,d=1/zoom))*np.pi*2\n",
        "    \n",
        "    # define the unnormalized sinc\n",
        "    sinc = lambda w : np.sinc(w/np.pi)  \n",
        "    \n",
        "    return w, sinc(w/2*photodiode_ratio)\n",
        "        \n",
        "\n",
        "# display zoom factor\n",
        "z = 2\n",
        "\n",
        "# relative size of the photodiode (usually < 1) \n",
        "photodiode_ratio=0.8\n",
        "\n",
        "w, Hsensor = sinc_OTF_profile(sh[0], z, photodiode_ratio)\n",
        "\n",
        "plt.figure()\n",
        "plt.grid('on')\n",
        "plt.title('frequency profile of the OTF')\n",
        "plt.vlines([-np.pi,np.pi], 0, 1) # Nyquist\n",
        "plt.plot(w, Hsensor)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE-IKlOfHjPX"
      },
      "source": [
        "Then we can use this profile to generate the OTF for the 2D sensor. \n",
        "We are interested in generating a zoomed version of the OTF so that it could be applied on the large image to simulate the sensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saKGxAFXHjPX"
      },
      "source": [
        "def sensor_OTF(sh,zoom,photodiode_ratio=1.0):\n",
        "    '''\n",
        "    Generates the zoomed version of the sensor OTF \n",
        "    Args:\n",
        "        sh: 2-element array with the shape [rows, cols] of the image to be generated\n",
        "        zoom: the oversampling factor to be considered \n",
        "        photodiode_ratio (default 1): is the size of the pixel relative to the separation\n",
        "    Returns:\n",
        "        the sensor OTF with the 0 shifted at coordinate at 0,0\n",
        "    '''\n",
        "    \n",
        "    _, Hcol = sinc_OTF_profile(sh[0], zoom, photodiode_ratio)\n",
        "    _, Hrow = sinc_OTF_profile(sh[1], zoom, photodiode_ratio)\n",
        "    \n",
        "    sinc = lambda w : np.sinc(w/np.pi)  # unnormalized sinc\n",
        "\n",
        "    H_sensor = (Hcol[np.newaxis,:]).transpose() @ Hrow[np.newaxis,:]\n",
        "    return ifftshift(H_sensor)   # put the zero frequency at it's place\n",
        "    \n",
        "\n",
        "    \n",
        "# oversampling factor \"zoom\"\n",
        "z = 2\n",
        "\n",
        "# relative size of the photodiode (usually < 1) \n",
        "photodiode_ratio=0.8\n",
        "     \n",
        "H_sensor = sensor_OTF(sh,z,photodiode_ratio)\n",
        "        \n",
        "plt.figure()\n",
        "plt.title('sensor OTF, 2x ')\n",
        "plt.imshow( fftshift(H_sensor) )\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgcVVf2BHjPX"
      },
      "source": [
        "### Diffraction OTF (circular aperture) \n",
        "\n",
        "\n",
        "A circular aperture with diameter $D$ produces a point-spread-function due to diffraction  called Airy disk \n",
        "\n",
        "<img width=150 src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Airy_disk_D65.png/220px-Airy_disk_D65.png\">\n",
        "\n",
        "\n",
        "Its OTF decreases with increasing spatial frequency and vanishes at the diffraction limit. \n",
        "<img width=400 src=\"https://i0.wp.com/www.strollswithmydog.com/wordpress/wp-content/uploads/Sampled-DiffSquare-with-Slice.png\">\n",
        "\n",
        "The diffraction limit  is given by the formula \n",
        "$$ \\rho_c = \\frac{2\\pi D }{\\lambda f}   \\quad [cycles/cm],  $$ \n",
        "where $\\lambda$ is the wavelength of light and $f$ is the focal lenght. \n",
        "\n",
        "This cutoff frequency corresponds to a samplig step equal to \n",
        "$$ \\delta_c = \\pi / \\rho_c = \\frac{\\lambda f}{2 D} \\quad [cm].$$\n",
        "\n",
        "The diffraction OTF profile is not exactly linear, its  form is given by the following formula which is normalized from 0 to the frequency cutoff \n",
        "$$ H_{\\text{diffraction}}(\\omega) =  \\frac{2}{\\pi}  \\left( \\arccos(\\rho(\\omega)) -  \\rho(\\omega)*\\sqrt{1-\\rho(w)^2} \\right)  \\quad \\text{with} \\quad \\rho(\\omega) = \\omega / \\rho_c.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nNXO79CHjPX"
      },
      "source": [
        "def diffraction_cutoff_frequency(D,f,lam):\n",
        "    \"\"\"\n",
        "    Computes the diffraction_cutoff_frequency for a circular aperture\n",
        "    Args:\n",
        "        D: diameter of the aperture\n",
        "        f: focal lenght \n",
        "        lam: wavelength of the light \n",
        "    All arguments must be expressed on the same linear units e.g. cm\n",
        "    Returns:\n",
        "        the curoff frequency expressed in cycles/cm, where a cycle is 2π\n",
        "    \"\"\"\n",
        "    return (2*np.pi*D)/(lam*f)\n",
        "\n",
        "    \n",
        "# difraction cutoff frequency \n",
        "cutoff_freq = diffraction_cutoff_frequency(D,f,lam)\n",
        "print('the diffraction cutoff frequency is:', cutoff_freq, 'cycles/cm')\n",
        "\n",
        "# the corresponding cycles per pixel  \n",
        "cutoff_freq_px = cutoff_freq/np.pi * pixel_pitch    #\n",
        "print('which amounts to:', cutoff_freq_px,   'cycles/px')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_mD_Z9sHjPX"
      },
      "source": [
        "def diffraction_OTF_profile(w):\n",
        "    \"\"\"\n",
        "    Evaluates the 1D normalized diffraction OTF \n",
        "    the diffraction cutoff is attained when w is 1, beyond that value 0 is returned\n",
        "    Args: \n",
        "        w: the normalized frequency \n",
        "    Returns: \n",
        "        the normalized diffraction OTF\n",
        "    \"\"\"\n",
        "    w = np.abs(w).clip(0,1)\n",
        "    return   np.where(w<1, 2/np.pi* ( np.arccos(w) -   w*np.sqrt(1-w**2)) , 0)\n",
        "\n",
        "\n",
        "# frequency range of the zoomed image [-z*pi, z*pi]\n",
        "#w = np.arange(-N/2,N/2)*2/N*z*np.pi\n",
        "w = fftshift(fftfreq(sh[0],d=1.0/z))*np.pi*2\n",
        "# normalized difraction OTF profile \n",
        "H_diff = diffraction_OTF_profile(w/np.pi)\n",
        "\n",
        "plt.figure()\n",
        "plt.grid('on')\n",
        "plt.title('normalized difraction OTF profile up to 2x the diffraction cutoff')\n",
        "plt.plot(w, H_diff)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZGKTjZDHjPY"
      },
      "source": [
        "We need to compare the spatial frequency of the our simulated sensor with the limit of the diffraction kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDHabmsKHjPY"
      },
      "source": [
        "print('The pixel step is:',  pixel_pitch, 'cm', '(or', 1/pixel_pitch, 'samples/cm)' )\n",
        "print('Given that the target pixel step is', pixel_pitch , 'cm, our zoomed image has a pixel of:', pixel_pitch/z, 'cm')\n",
        "print('')\n",
        "\n",
        "max_frequency_zoomed_image = np.pi/ (pixel_pitch/z)\n",
        "print('Thus the maximum spatial frequency visible with our zoomed pixels is', max_frequency_zoomed_image,'cycles/cm')\n",
        "print('while the spatial frequency of the diffraction cutoff  is', cutoff_freq,'cycles/cm')\n",
        "print('')\n",
        "\n",
        "\n",
        "relative_cutoff = cutoff_freq/max_frequency_zoomed_image\n",
        "print('In conclusion, the diffraction cutoff frequency is', relative_cutoff, 'times the one of the zoomed image' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXr9e_zoHjPY"
      },
      "source": [
        "We  can now simulate the diffraction OTF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWmUu-mVHjPY"
      },
      "source": [
        "def diffraction_OTF(sh,D,f,lam,pixel_pitch,zoom):\n",
        "    '''\n",
        "    Generates the diffraction OTF for a zoomed image, simulating a specific pupil diameter D, focal lenght f, and pixel_pitch\n",
        "    Args:\n",
        "        sh: 2-element array with the shape [rows, cols] of the image to be generated      \n",
        "        D: diameter of the aperture\n",
        "        f: focal lenght \n",
        "        lam: wavelength of the light to be used \n",
        "        zoom: the oversampling factor to be considered \n",
        "        \n",
        "    D,f,lam,pixel_pitch: must be expressed on the same linear units e.g. cm\n",
        "\n",
        "    Returns:\n",
        "        the diffraction OTF with the 0 shifted at coordinate at 0,0\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    # difraction cutoff frequency \n",
        "    cutoff_freq =  diffraction_cutoff_frequency(D,f,lam)\n",
        "\n",
        "    # sensor cutoff frequency\n",
        "    max_frequency_zoomed_image = np.pi/ (pixel_pitch/zoom)\n",
        "\n",
        "    relative_cutoff = cutoff_freq/max_frequency_zoomed_image\n",
        "    \n",
        "    # the frequency range of the output image [-z*pi, z*pi]^2\n",
        "    #wx = np.arange(-sh[1]/2,sh[1]/2)*zoom*np.pi*2/sh[1]\n",
        "    #wy = np.arange(-sh[0]/2,sh[0]/2)*zoom*np.pi*2/sh[0]\n",
        "    wx = fftshift(fftfreq(sh[1],d=1.0/zoom))*np.pi*2\n",
        "    wy = fftshift(fftfreq(sh[0],d=1.0/zoom))*np.pi*2\n",
        "    X,Y = np.meshgrid(wx,wy)\n",
        "\n",
        "    # diffraction profile \n",
        "    H = diffraction_OTF_profile( np.hypot(X,Y)/ (zoom*np.pi) /relative_cutoff)\n",
        "          \n",
        "    return  ifftshift( H )\n",
        "\n",
        "  \n",
        "H_diffraction  = diffraction_OTF(sh,D,f,lam,pixel_pitch,2)\n",
        "\n",
        "plt.title('diffraction OTF, 2x')\n",
        "plt.imshow( fftshift( H_diffraction) )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB7rPu3uHjPY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYEXengTHjPY"
      },
      "source": [
        "### Simulation of the system\n",
        "\n",
        "\n",
        "Putting together the previous steps we can simulate our sysmtem:\n",
        "\n",
        "$$ v =  \\Delta   ( T  ( K_{\\text{diffraction}} \\ast K_{\\text{sensor}}\\ast u )  ) + \\text{noise}$$ \n",
        "\n",
        "1. take a high resolution image and a fix a zoom factor (the simulated image will be z times smaller)\n",
        "2. generate the high resolution sensor and diffraction OTF for corresponding to the current system\n",
        "3. apply the convolution ot the high resolution image\n",
        "4. generate a sampling grid and sample the high resolution image (here we will only consider grids that correspond to translations)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ8kxE6vHjPZ"
      },
      "source": [
        "# skysat's parameters \n",
        "\n",
        "D=35      #cm \n",
        "f=360     #cm = 3.6 m\n",
        "lam=6e-5  #cm = 0.6 micrometers   # ~visible light \n",
        "pixel_pitch = 6.5e-4 #cm   6.5 µm\n",
        "alt = 50000000 # 500km\n",
        "photodiode_ratio=.8  # not the actual value \n",
        "z=2       # zoom factor of the high resolution image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeZT3G7OHjPZ"
      },
      "source": [
        "def simulate_LR(u, H, dx, dy, z = 2, sigma=0):\n",
        "    \"\"\"\n",
        "    Simulate a low resolution image from the high resolution image \n",
        "        v = Pi_z (  T_{dx,dy} ( H*u ) ) + N(sigma,0) \n",
        "    Args:\n",
        "        u: input high resolution image\n",
        "        H: applying the OTF\n",
        "        dx,dy: shifts the sampling grid by (dx,dy)\n",
        "        z: integer subsampling factor  (default 2) \n",
        "        sigma: std of the noise to be added to the samples \n",
        "    Returns:\n",
        "        the simulated image, \n",
        "        the X,Y coordinates of the samples, \n",
        "        and a binary mask indicating if the samples are invalid\n",
        "    \"\"\"\n",
        "    \n",
        "    from sr import fft_translate\n",
        "    h, w = u.shape\n",
        "\n",
        "    # precompute the positions of the samples \n",
        "    X, Y = np.meshgrid(np.arange(w).astype(float), np.arange(h).astype(float))\n",
        "\n",
        "    # apply the OTF  \n",
        "    u_bl = np.real(ifft2(fft2(u) * H))\n",
        "\n",
        "    # translate and subsample \n",
        "    u_lr = fft_translate(u_bl, dx, dy)[::z,::z]\n",
        "    # do the same with the sample positions \n",
        "    Xs = (X[::z,::z]-dx)/2\n",
        "    Ys = (Y[::z,::z]-dy)/2\n",
        "    \n",
        "    # add noise \n",
        "    u_lr +=  np.random.randn(*u_lr.shape)*sigma\n",
        "    \n",
        "    \n",
        "    # determine mask \n",
        "    Ms =  ((Xs[0,:]>0)  *  (Xs[0,:] <= w) *  (Ys[1,:] >0) * (Ys[1,:] <= h) )\n",
        "    \n",
        "    \n",
        "    return u_lr,Xs,Ys,Ms\n",
        "\n",
        "\n",
        "sh = u.shape\n",
        "dx,dy = 0.3,0.6\n",
        "# generate the OTF \n",
        "H_sensor = sensor_OTF(sh,z,photodiode_ratio=photodiode_ratio)\n",
        "H_diffraction =  diffraction_OTF(sh,D,f,lam,pixel_pitch,z)\n",
        "\n",
        "# plot the profile of the simulated OTF \n",
        "plt.figure()\n",
        "plt.vlines([sh[1]/4,3*sh[1]/4], 0, 1) # Nyquist\n",
        "plt.plot(  fftshift((H_sensor * H_diffraction)[0,:] ) )\n",
        "\n",
        "\n",
        "# simulate the LR image \n",
        "v, Xs, Ys, Ms = simulate_LR(u, H_sensor * H_diffraction, dx, dy)\n",
        "\n",
        "# high resolution but blurred image\n",
        "u_bl = ifft2(fft2(u)* H_sensor  * H_diffraction )\n",
        "imshowfft( u_bl ,z, title='high resolution filtered' )\n",
        "\n",
        "\n",
        "# subsampled image\n",
        "imshowfft( v , title='subsampled')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UBdyWtoHjPZ"
      },
      "source": [
        "# III. Fusion\n",
        "\n",
        "The fusion steps takes a set of aliased images (that are aligned on the same reference)  and fuse the samples to produce a higher resolution image.\n",
        "\n",
        "We first produce a set of images using the simulator from the previous section. Then we will compare two fusion techniques:\n",
        "* Shift-and-Add, which accumulates the samples of the images to the nearest neighbor of the high resolution image and\n",
        "* ACT, which fits a trigonometric polynomial model to the samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlhe0GamHjPZ"
      },
      "source": [
        "### Generate a sequence of LR frames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tD2rqDdHjPZ"
      },
      "source": [
        "def simulate_LR_stack_sanity_check(u, H):\n",
        "    \"\"\"\n",
        "    This function simulates only one sampling with 4 integer positions \n",
        "    \"\"\"\n",
        "    ## sanity check ! \n",
        "    shifts = np.array([[0,0],[0,-1],[-1,0],[-1,-1]])\n",
        "\n",
        "    # generate the stack \n",
        "    Xs = []\n",
        "    Ys = []\n",
        "    Vs = []\n",
        "    Ms = []\n",
        "    for i,(dx,dy) in enumerate(shifts) :\n",
        "        print(i,dx,dy)\n",
        "        V,X,Y,M = simulate_LR(u, H, dx, dy, z=2, sigma=0)\n",
        "\n",
        "        # refer the shifts are already referred to the subsampled images\n",
        "        Xs.append(X)\n",
        "        Ys.append(Y)\n",
        "        Vs.append(V)\n",
        "        Ms.append(M)\n",
        "\n",
        "    Xs=np.array(Xs)  \n",
        "    Ys=np.array(Ys)  \n",
        "    Vs=np.array(Vs)  \n",
        "    Ms=np.array(Ms)  \n",
        "\n",
        "    return Vs,Xs,Ys,Ms\n",
        "\n",
        "\n",
        "\n",
        "def simulate_LR_stack(u, H, nframes, z=2, sigma=0, jitter=0):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    \n",
        "    ## random shifts forcing the first one to be 0,0\n",
        "    shifts = np.random.rand(nframes,2)*4 -2\n",
        "    shifts[0] = [0,0]\n",
        "\n",
        "    # generate the stack \n",
        "    Xs = []\n",
        "    Ys = []\n",
        "    Vs = []\n",
        "    Ms = []\n",
        "    for i,(dx,dy) in enumerate(shifts) :\n",
        "        #print(i,dx,dy)\n",
        "        V,X,Y,M = simulate_LR(u, H, dx, dy, z=z, sigma=sigma)\n",
        "        # add jitter\n",
        "        X += np.random.randn(1)*jitter\n",
        "        Y += np.random.randn(1)*jitter\n",
        "        # refer the shifts are already referred to the subsampled images\n",
        "        Xs.append(X)\n",
        "        Ys.append(Y)\n",
        "        Vs.append(V)\n",
        "        Ms.append(M)\n",
        "\n",
        "    Xs=np.array(Xs)  \n",
        "    Ys=np.array(Ys)  \n",
        "    Vs=np.array(Vs)  \n",
        "    Ms=np.array(Ms)  \n",
        "\n",
        "    return Vs,Xs,Ys,Ms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8wvjk1aHjPZ"
      },
      "source": [
        "# the input image u\n",
        "u = utils.readGTIFF('data/barb.png').squeeze()[100:356,100:356]\n",
        "sh = u.shape\n",
        "\n",
        "H_sensor      = sensor_OTF(sh,z,photodiode_ratio=photodiode_ratio)\n",
        "H_diffraction =  diffraction_OTF(sh,D,f,lam,pixel_pitch,z)\n",
        "Vs,Xs,Ys,Ms = simulate_LR_stack(u, H_sensor*H_diffraction, 10, z = 2, sigma=0)\n",
        "\n",
        "vistools.display_gallery(Vs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sr0PEDYHjPZ"
      },
      "source": [
        "### Shift-and-Add \n",
        "\n",
        "The shift-and-add accumulates the samples of the images to the nearest neighbor of the high resolution image. This process might leave some pixels without votes, so a last postprocess fills the missing pixels with the median of its neighbors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yei632jMHjPZ"
      },
      "source": [
        "def shift_and_add(outshape, Vs, Xs, Ys, Ms=None, fill_missing=False):\n",
        "    \"\"\"\n",
        "    Apply the shift-and-add fusion algorithm\n",
        "    Args:\n",
        "        outshape: 2-element array with the size of the output image\n",
        "        Vs: array containing the sample values\n",
        "        Xs: array containing the x pixel coordinate of the samples \n",
        "        Ys: array containing the y pixel coordinate of the samples \n",
        "        Ms: array containing the masks of valid samples \n",
        "    Returns:\n",
        "        np.ndarray: of size outshape with the resulting image\n",
        "    \"\"\"\n",
        "  \n",
        "    if Ms is None:\n",
        "        Ms = np.ones_like(Ys)\n",
        "    Ms = Ms > 0  # make it bool\n",
        "\n",
        "    h, w, = outshape\n",
        "    acc  = np.zeros((h,w))\n",
        "    count = np.zeros((h,w))\n",
        "    \n",
        "    # accumulate the pixels of each image Vs[i] on acc and count\n",
        "    for i in range(Vs.shape[0]):\n",
        "        Mp = Ms[i]\n",
        "        Vp = Vs[i][Mp]\n",
        "        Xp = Xs[i][Mp]\n",
        "        Yp = Ys[i][Mp]\n",
        "        \n",
        "        Xp = (np.clip(np.round(Xp),0,w-1)).astype(int)\n",
        "        Yp = (np.clip(np.round(Yp),0,h-1)).astype(int)\n",
        "        \n",
        "        acc[Yp,Xp]   += Vp\n",
        "        count[Yp,Xp] += np.ones_like(Vp)\n",
        "        \n",
        "    # catch the missing pixels \n",
        "    missing = np.where(count==0)\n",
        "    count[missing] = 1\n",
        "    \n",
        "    # compute averages\n",
        "    out = acc/count\n",
        "    \n",
        "    # fill-in the gaps with a median filter \n",
        "    if fill_missing and np.sum(missing)>0:\n",
        "        #out[missing] = scipy.ndimage.median_filter(out, 3)[missing]\n",
        "        ### Ideally one should ignore the missing pixels in the median\n",
        "        ### but the following lines do not seem to work in all platforms\n",
        "        out[missing] = np.nan\n",
        "        out[missing] = scipy.ndimage.generic_filter(out, np.nanmedian, 3)[missing]\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wluTYZYHjPa"
      },
      "source": [
        "h,w = Vs[0].shape\n",
        "zz  = 2\n",
        "u_sa = shift_and_add( [int(h*zz),int(w*zz)], Vs, Xs*zz, Ys*zz, Ms)\n",
        "\n",
        "imshowfft(u_sa, title='S&A', z=zz)\n",
        "imshowfft(u_bl, title='ideal' )\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(u_sa-np.real(u_bl) ,vmin=-10, vmax=10);plt.title('difference'); plt.colorbar()\n",
        "vistools.display_gallery([u_sa, u_bl])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZTuuXT1HjPa"
      },
      "source": [
        "### The ACT algorithm\n",
        "\n",
        "\n",
        "The ACT algorithm (Adaptive-weights Conjugate-gradient on Toeplitz-matrix) proposes to find the coefficients of a trigonometric polynomial $\\hat u$ that solve the variational problem\n",
        "$$ \\min_{\\hat u} \\|S \\hat u  -  z \\|^2,$$ \n",
        "where $S$ denotes the sampling operator and $z$ are the observed samples. \n",
        "This yields the normal equations  $ S^* S \\hat u = S^* z $  which can be solved by conjugate gradient. \n",
        "\n",
        "Adding a regularization term of the form $ \\lambda \\| D \\hat u \\|^2$, where $D$ can be a differential operator (e.g. the gradient), which is diagonal in the frequency domain, leads a similar linear system\n",
        "$$ S^* S \\hat u  + \\lambda D^* D  \\hat u  = S^* z.$$\n",
        "\n",
        "\n",
        "The particularity of the ACT algorithm is that it exploits a property of $S^* S$, which is Toeplitz, to accelerate its evaluation. For sake of simplicity, here we show a simpler version that use the $S^*$ and $S$ at each iteration. The implementation using the Toeplitz trick is available in the module `sr`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpXeRLXyHjPa"
      },
      "source": [
        "from nfft_tools import initS, applyS, applySadj\n",
        "from scipy.sparse.linalg import LinearOperator, cg \n",
        "from numpy.fft import fftshift, ifftshift, fft2, ifft2\n",
        "import scipy.sparse.linalg\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def ACT_nfft(samples, irreg, N, maxiter=10, lam=0):\n",
        "    \n",
        "    #  regularization penalty in the frequency domain\n",
        "    DDx, DDy = np.meshgrid( np.pi*  (np.arange(N[1], dtype=np.double) - N[1]//2 )/(N[1]) ,  \n",
        "                            np.pi*  (np.arange(N[0], dtype=np.double) - N[0]//2 )/(N[0]) )\n",
        "    LAP = lam * (DDx**2 + DDy**2)\n",
        "\n",
        "\n",
        "    # initialize NFFT\n",
        "    nfftplan = initS(samples, N)\n",
        "    \n",
        "    # apply adjoint sampling to compute b\n",
        "    b = applySadj(irreg ,nfftplan).flatten()\n",
        "\n",
        "    \n",
        "    # prepare the linear operator  S^* S (which is Toeplitz)\n",
        "    def StS(v):\n",
        "        t0 = v.astype(np.complex).reshape(N)\n",
        "        t1 = applyS(t0,nfftplan) \n",
        "        t2 = applySadj(t1,nfftplan)\n",
        "        if lam > 0:\n",
        "            t2 +=  LAP * t0\n",
        "        return t2.flatten()\n",
        "    A = LinearOperator((N[0]*N[1],N[0]*N[1]), matvec=StS, dtype=np.complex)\n",
        "\n",
        "    \n",
        "    # prepare the CG to solve Ax=b \n",
        "    def cb(x):\n",
        "        cb.cgiter += 1\n",
        "        if cb.cgiter % 10 == 0:\n",
        "            print('cg iterations... %d'%cb.cgiter)\n",
        "    cb.cgiter = 0\n",
        "    x = scipy.sparse.linalg.cg(A, b, maxiter=maxiter, callback=cb)[0]\n",
        "\n",
        "    tmp = x.reshape((N[0],N[1])) \n",
        "\n",
        "    tmp =  np.real(ifft2 (fftshift(tmp.conj())))\n",
        "\n",
        "    return np.real(tmp)\n",
        "\n",
        "\n",
        "\n",
        "def ACT(outshape, Vs, Xs, Ys, Ms=None):\n",
        "    \"\"\"\n",
        "    Apply the ACT fusion algorithm (simplified interface)\n",
        "    Args:\n",
        "        outshape: 2-element array with the size of the output image\n",
        "        Vs: array containing the sample values\n",
        "        Xs: array containing the x pixel coordinate of the samples \n",
        "        Ys: array containing the y pixel coordinate of the samples \n",
        "        Ms: array containing the masks of valid samples \n",
        "    Returns:\n",
        "        np.ndarray: of size outshape with the resulting image\n",
        "    \"\"\"\n",
        "    h,w = outshape\n",
        "    \n",
        "    if Ms is None:\n",
        "        Ms = np.ones_like(Ys)\n",
        "    Ms = (Ms > 0)  # make it bool\n",
        "    \n",
        "    samplesloc =  np.vstack(( np.array(Ys[Ms]).flatten()/h  ,  np.array(Xs[Ms]).flatten()/w  )).T\n",
        "    irregVal = np.array(Vs[Ms]).flatten()\n",
        "    \n",
        "    out = ACT_nfft(samplesloc, irregVal, outshape, maxiter=30, lam=0.01)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgQK_kUAHjPa"
      },
      "source": [
        "h,w = Vs[0].shape\n",
        "zz  = 2\n",
        "u_act = ACT( [int(h*zz),int(w*zz)], Vs, Xs*zz, Ys*zz)\n",
        "\n",
        "imshowfft(u_act, title='ACT', z=2)\n",
        "imshowfft(u_bl, title='ideal image' )\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(u_act-np.real(u_bl) ,vmin=-10, vmax=10); plt.title('difference'); plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp4ZqdBZHjPa"
      },
      "source": [
        "Now that we've seen that ACT works, let's use its faster version from the `sr` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tepGZBwiHjPa"
      },
      "source": [
        "from sr import ACT_nfft"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7erohLfHjPa"
      },
      "source": [
        "### Deconvolution / sharpening\n",
        "\n",
        "Since we restore the image convolved with the diffraction and sensor kernels it is reasonable that it looks a bit blurry. The following block applies a simple sharpening with the objective of improving the contrast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4KLvNyKHjPb"
      },
      "source": [
        "#u_act = ACT( [h*zz,w*zz], Vs, Xs*zz, Ys*zz)\n",
        "def sharpen(u, alpha=1):\n",
        "    \"\"\"\n",
        "    sharpen the image u by adding the discrete laplacian\n",
        "    \"\"\"\n",
        "    import scipy.signal\n",
        "    return u + alpha * scipy.signal.convolve (u, np.array([[1,-2,1],[-2,4,-2],[1,-2,1]])/4 ,mode='same' ) \n",
        "    \n",
        "u_act1 = sharpen(u_act)\n",
        "\n",
        "u_zp = fft_zoom(Vs[0],2)\n",
        "\n",
        "vistools.display_gallery( [ utils.simple_equalization_8bit(x) for x in [u_act, u_act1, u_bl, u_zp] ] , ['ACT','ACT sharpened', 'filtered input', 'zero padded'])\n",
        "imshowfft(utils.simple_equalization_8bit(u_act1), figsize=(20,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0skuCqJHjPb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKtfR4ujHjPb"
      },
      "source": [
        "# IV. Registration\n",
        "\n",
        "\n",
        "The registration \n",
        "\n",
        "* load the frames \n",
        "* prefilter the frames for better alignement \n",
        "* generate a sequence of homographies that register theimagesiiiiiiihow the aligned stack of images\n",
        "\n",
        "\n",
        "\n",
        "We provide three real sequences: \n",
        "\n",
        "        files = glob(\"data/mire/crop*.tif\")\n",
        "        files = glob(\"data/buildings/smallim_*.tiff\")\n",
        "        files = glob(\"data/classic/smallim_*.tiff\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8ub8KYQHjPb"
      },
      "source": [
        "def ECC_frame_align(im1_gray, im2_gray, gaussFiltSize=1):\n",
        "    \"\"\"\n",
        "    Compute a parametric registration (homography) of image2 over image1 using the ECC algorithm\n",
        "    Adapted from: https://www.geeksforgeeks.org/image-registration-using-opencv-python/\n",
        "    Args: \n",
        "        im1_gray: target image \n",
        "        im2_gray: image to be registered \n",
        "        gaussFiltSize: (int>1) size of the gaussian prefilter \n",
        "    Returns: \n",
        "        a resampled version of im2_gray registered over im1_gray\n",
        "        the 3x3 matrix describing the transformation     \n",
        "    \"\"\"\n",
        "    import cv2\n",
        "    import scipy.ndimage\n",
        "    import numpy as np\n",
        "\n",
        "    # Find size of image1\n",
        "    sz = im1_gray.shape\n",
        "\n",
        "    # Define the motion model\n",
        "    warp_mode = cv2.MOTION_TRANSLATION\n",
        "    warp_mode = cv2.MOTION_AFFINE\n",
        "    warp_mode = cv2.MOTION_HOMOGRAPHY\n",
        "\n",
        "    # Define 2x3 or 3x3 matrices and initialize the matrix to identity\n",
        "    if warp_mode == cv2.MOTION_HOMOGRAPHY :\n",
        "        warp_matrix = np.eye(3, 3, dtype=np.float32)\n",
        "    else :\n",
        "        warp_matrix = np.eye(2, 3, dtype=np.float32)\n",
        "\n",
        "    # Specify the number of iterations.\n",
        "    number_of_iterations = 30;\n",
        "\n",
        "    # Specify the threshold of the increment\n",
        "    # in the correlation coefficient between two iterations\n",
        "    termination_eps = 1e-10;\n",
        "\n",
        "    # Define termination criteria\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, number_of_iterations,  termination_eps)\n",
        "\n",
        "    # Prefilter the images to remove alias\n",
        "    im1_grayB = scipy.ndimage.gaussian_filter(im1_gray.astype(np.float32), gaussFiltSize)\n",
        "    im2_grayB = scipy.ndimage.gaussian_filter(im2_gray.astype(np.float32), gaussFiltSize)\n",
        "    \n",
        "    # Run the ECC algorithm. The results are stored in warp_matrix.\n",
        "    (cc, warp_matrix) = cv2.findTransformECC(im1_grayB, im2_grayB,\n",
        "                                             warp_matrix, warp_mode, \n",
        "                                             criteria, None)\n",
        "\n",
        "    # just convert all warp matrices into Homographies\n",
        "    if warp_matrix.shape[0] == 2:\n",
        "        warp_matrix = np.vstack([warp_matrix,[0,0,1]])\n",
        "    \n",
        "    # Use warpPerspective for Homography\n",
        "    im2_gray_aligned = cv2.warpPerspective (im2_gray, warp_matrix, (sz[1],sz[0]), flags=cv2.INTER_LANCZOS4 + cv2.WARP_INVERSE_MAP)\n",
        "    ## Use warpAffine for Translation, Euclidean and Affine\n",
        "    #im2_gray_aligned = cv2.warpAffine(im2_gray, warp_matrix, (sz[1],sz[0]), flags=cv2.INTER_LANCZOS4 + cv2.WARP_INVERSE_MAP);\n",
        "        \n",
        "    return im2_gray_aligned, warp_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuRZniO_HjPb"
      },
      "source": [
        "from glob import glob\n",
        "\n",
        "files = glob(\"data/mire/crop*.tif\")\n",
        "files = glob(\"data/classic/smallim_*.tiff\")\n",
        "files = glob(\"data/buildings/smallim_*.tiff\")\n",
        "files = sorted(files)\n",
        "\n",
        "\n",
        "# Read the images to be aligned\n",
        "im1 = utils.readGTIFF(files[0]).squeeze()\n",
        "h,w = im1.shape\n",
        "\n",
        "gaussFiltSize = 1\n",
        "\n",
        "imgs = []\n",
        "transformations = []\n",
        "\n",
        "for fname in files:\n",
        "    # read the image \n",
        "    im2 = utils.readGTIFF(fname).squeeze()\n",
        "    # align with the reference \n",
        "    im2_aligned, warp_matrix = ECC_frame_align(im1, im2, gaussFiltSize=gaussFiltSize)\n",
        "    \n",
        "    # append the result\n",
        "    imgs.append(im2_aligned)\n",
        "    transformations.append(warp_matrix)\n",
        "    \n",
        "    \n",
        "vistools.display_gallery([ utils.simple_equalization_8bit(x) for x in  imgs ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cUkX30HjPb"
      },
      "source": [
        "# V. Super resolution of real satellite images \n",
        "\n",
        "* compute the image registration homographies \n",
        "* generate the sample list \n",
        "* call the fusion algorithm\n",
        "* sharpen the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZrnzMC7HjPb"
      },
      "source": [
        "def convert_homography_into_samples(warp_matrix, shape):\n",
        "    \"\"\"\n",
        "    apply the homography transformation (warp_matrix, 3x3) to the sample positions of an image of size (shape)\n",
        "    returns the sample positions X, Y and a binary mask M  indicating which pixels fell outside of the image\n",
        "    \"\"\"\n",
        "    h,w = shape\n",
        "\n",
        "    # apply transformation to the samples \n",
        "    X, Y = np.meshgrid(np.arange(w), np.arange(h))\n",
        "    X, Y = X.flatten(), Y.flatten()\n",
        "    \n",
        "    samplesloc = np.vstack(( X , Y, np.ones_like(X) ))\n",
        "    newsamplesloc = np.linalg.inv(warp_matrix)@ samplesloc\n",
        "    newsamplesloc[0,:]/=newsamplesloc[2,:]\n",
        "    newsamplesloc[1,:]/=newsamplesloc[2,:]\n",
        "\n",
        "    # determine mask \n",
        "    mask =  ((newsamplesloc[0,:]>0)  *  (newsamplesloc[0,:] <= w) *  (newsamplesloc[1,:] >0) * (newsamplesloc[1,:] <= h) )\n",
        "    \n",
        "    Xs = newsamplesloc[0,:].reshape(shape)\n",
        "    Ys = newsamplesloc[1,:].reshape(shape)\n",
        "    Ms = mask.reshape(shape)\n",
        "    \n",
        "    return Xs, Ys, Ms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axOufGZsHjPb"
      },
      "source": [
        "from glob import glob\n",
        "\n",
        "files = glob(\"data/mire/crop*.tif\")\n",
        "files = glob(\"data/buildings/smallim_*.tiff\")\n",
        "files = glob(\"data/classic/smallim_*.tiff\")\n",
        "files = sorted(files)\n",
        "\n",
        "\n",
        "\n",
        "# Read the images to be aligned\n",
        "im1 = utils.readGTIFF(files[0]).squeeze()\n",
        "h,w = im1.shape\n",
        "\n",
        "gaussFiltSize = 5\n",
        "\n",
        "warped = []\n",
        "transformations = []\n",
        "\n",
        "Xs=[] # sample x coordinate\n",
        "Ys=[] # sample y coordinate\n",
        "Vs=[] # sample value\n",
        "Ms=[] # sample mask: true valid, false invalid\n",
        "\n",
        "for fname in files:\n",
        "    # read the image \n",
        "    im2 = utils.readGTIFF(fname).squeeze()\n",
        "    # align with the reference \n",
        "    im2_aligned, warp_matrix = ECC_frame_align(im1, im2, gaussFiltSize=gaussFiltSize)\n",
        "\n",
        "    # append the result\n",
        "    warped.append(im2_aligned)\n",
        "    transformations.append(warp_matrix)\n",
        "\n",
        "    # apply transformation to the samples \n",
        "    X,Y,M = convert_homography_into_samples(warp_matrix, im1.shape)\n",
        "     \n",
        "    # concatenate with the samples \n",
        "    Xs.append(X)\n",
        "    Ys.append(Y)\n",
        "    Vs.append(im2)\n",
        "    Ms.append(M)\n",
        "    \n",
        "Xs = np.array(Xs)\n",
        "Ys = np.array(Ys)\n",
        "Vs = np.array(Vs)\n",
        "Ms = np.array(Ms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBnzmqhfHjPc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "vistools.display_gallery([ utils.simple_equalization_8bit(x) for x in  warped ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9JKEJyCHjPc"
      },
      "source": [
        "h,w = Vs[0].shape\n",
        "zz  = 2\n",
        "\n",
        "u_sa  = shift_and_add( [int(h*zz),int(w*zz)], Vs, Xs*zz, Ys*zz, Ms, fill_missing=True)\n",
        "u_act = ACT( [int(h*zz),int(w*zz)], Vs, Xs*zz, Ys*zz, Ms)\n",
        "\n",
        "u_sa1  = sharpen(u_sa)\n",
        "u_act1 = sharpen(u_act)\n",
        "\n",
        "imshowfft(u_sa1,  title='S&A sharpened', z=zz, figsize=(20,20) )\n",
        "imshowfft(u_act1, title='ACT sharpened', z=zz, figsize=(20,20))\n",
        "\n",
        "vistools.display_gallery([ utils.simple_equalization_8bit(x) for x in [u_sa1, u_act1] ], ['S&A sharpened', 'ACT sharpened'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnDZobuYHjPc"
      },
      "source": [
        "#### Apply the above procedure to another senquence of images\n",
        "\n",
        "\n",
        "\n",
        "> **<u>Excercise 3</u>**: Super-resolve a different set of images (from the ones provided)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqd52AX-HjPc"
      },
      "source": [
        "### EXERCISE. WRITE YOUR SOLUTION CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "### EXERCISE. WRITE YOUR SOLUTION CODE HERE\n",
        "\n",
        "# BEGIN SOLUTION\n",
        "\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "files = glob(\"data/mire/crop*.tif\")\n",
        "files = glob(\"data/buildings/smallim_*.tiff\")\n",
        "files = glob(\"data/classic/smallim_*.tiff\")\n",
        "files = sorted(files)\n",
        "\n",
        "\n",
        "def align_frames(files, gaussFiltSize=5):\n",
        "\n",
        "    # Read the images to be aligned\n",
        "    im1 = utils.readGTIFF(files[0]).squeeze()\n",
        "    h,w = im1.shape\n",
        "\n",
        "    gaussFiltSize = 5\n",
        "\n",
        "    warped = []\n",
        "    transformations = []\n",
        "\n",
        "    Xs=[] # sample x coordinate\n",
        "    Ys=[] # sample y coordinate\n",
        "    Vs=[] # sample value\n",
        "    Ms=[] # sample mask: true valid, false invalid\n",
        "\n",
        "    for fname in files:\n",
        "        # read the image \n",
        "        im2 = utils.readGTIFF(fname).squeeze()\n",
        "        # align with the reference \n",
        "        im2_aligned, warp_matrix = ECC_frame_align(im1, im2, gaussFiltSize=gaussFiltSize)\n",
        "            \n",
        "        # append the result\n",
        "        warped.append(im2_aligned)\n",
        "        transformations.append(warp_matrix)\n",
        "\n",
        "        # apply transformation to the samples \n",
        "        X,Y,M = convert_homography_into_samples(warp_matrix, im1.shape)\n",
        "        \n",
        "        # concatenate with the samples \n",
        "        Xs.append(X)\n",
        "        Ys.append(Y)\n",
        "        Vs.append(im2)\n",
        "        Ms.append(M)\n",
        "        \n",
        "    Xs = np.array(Xs)\n",
        "    Ys = np.array(Ys)\n",
        "    Vs = np.array(Vs)\n",
        "    Ms = np.array(Ms)\n",
        "\n",
        "    return Xs,Ys,Vs,Ms,warped \n",
        "\n",
        "\n",
        "Xs,Ys,Vs,Ms,warped  = align_frames(files, gaussFiltSize=5)\n",
        "\n",
        "vistools.display_gallery([ utils.simple_equalization_8bit(x) for x in  warped ] )\n",
        "\n",
        "\n",
        "h,w = Vs[0].shape\n",
        "zz  = 2\n",
        "\n",
        "u_sa  = shift_and_add( [int(h*zz),int(w*zz)], Vs, Xs*zz, Ys*zz, Ms, fill_missing=True)\n",
        "u_act = ACT( [int(h*zz),int(w*zz)], Vs, Xs*zz, Ys*zz, Ms)\n",
        "\n",
        "u_sa1  = sharpen(u_sa)\n",
        "u_act1 = sharpen(u_act)\n",
        "\n",
        "imshowfft(u_sa1,  title='S&A sharpened', z=zz, figsize=(20,20) )\n",
        "imshowfft(u_act1, title='ACT sharpened', z=zz, figsize=(20,20))\n",
        "\n",
        "vistools.display_gallery([ utils.simple_equalization_8bit(x) for x in [u_sa1, u_act1] ], ['S&A sharpened', 'ACT sharpened'])\n",
        "\n",
        "# END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_ac4FqHHjPc"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "Here we study how the presented methods behave with different number of input images,  in presence of jitter (error in the sample positions), and  image noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM10XByaHjPc"
      },
      "source": [
        "def RMSE(A,B,pad=20):\n",
        "    return np.sqrt( ((A[pad:-pad,pad:-pad] - B[pad:-pad,pad:-pad])**2).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8IDamzdHjPc"
      },
      "source": [
        "> **<u>Exercise 4:</u>** analyze the impact of alignment errors (jitter) in the restorations for S&A and ACT methods. Plot the RMSE vs jitter error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv_SBYQ1HjPd"
      },
      "source": [
        "### EXERCISE. WRITE YOUR SOLUTION CODE HERE\n",
        "\n",
        "\n",
        "# BEGIN SOLUTION\n",
        "sh = u.shape\n",
        "H_sensor = sensor_OTF(sh,z,photodiode_ratio=photodiode_ratio)\n",
        "H_diffraction =  diffraction_OTF(sh,D,f,lam,pixel_pitch,z)\n",
        "H = H_sensor * H_diffraction\n",
        "\n",
        "u_bl = np.real(ifft2(fft2(u) * H))\n",
        "\n",
        "outs_sa = []\n",
        "rmse_sa = []\n",
        "outs_act = []\n",
        "rmse_act = []\n",
        "jitters  = 10** (np.linspace(-4,1,20))\n",
        "for jj in jitters:\n",
        "    \n",
        "    Vs,Xs,Ys,Ms = simulate_LR_stack(u, H, 20, z = 2, sigma=0, jitter=jj)\n",
        "    \n",
        "    h,w = Vs[0].shape\n",
        "    \n",
        "    u_act = ACT( [int(h*z),int(w*z)], Vs, Xs*z, Ys*z,Ms)\n",
        "    u_sa = shift_and_add( [int(h*z),int(w*z)], Vs, Xs*z, Ys*z,Ms)\n",
        "\n",
        "    outs_act.append(u_act)\n",
        "    outs_sa.append(u_sa)\n",
        "\n",
        "    rmse_act.append( RMSE (u_bl, u_act) )\n",
        "    rmse_sa.append( RMSE (u_bl, u_sa))\n",
        "\n",
        "    \n",
        "plt.loglog(jitters, rmse_act,label='act')\n",
        "plt.loglog(jitters, rmse_sa,label='sa')\n",
        "plt.legend()\n",
        "\n",
        "vistools.display_gallery(outs_act)\n",
        "# END SOLUTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsAqJ3KLHjPd"
      },
      "source": [
        "> **<u>Exercise 5:</u>** analyze the impact of noise in the restoration. Plot RMSE vs sigma of noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWI_RqALHjPd"
      },
      "source": [
        "### EXERCISE. WRITE YOUR SOLUTION CODE HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h_bP7rFHjPd"
      },
      "source": [
        "> **<u>Exercise 6:</u>** analyze the effect of having more images in the restoration. Plot RMSE vs number of images (e.g. `np.range(2,30,3)`) for the methods S&A and ACT and for different sigma levels (e.g. $\\sigma=$ 0.3 and 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9SWtoAQHjPd"
      },
      "source": [
        "### EXERCISE. WRITE YOUR SOLUTION CODE HERE\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdgxqR3JnaY6"
      },
      "source": [
        "---------------------------\n",
        "[//]: # (© 2021 Gabriele Facciolo)\n",
        "[//]: # (<div style=\"text-align:center; font-size:75%;\"> Copyright © 2021 Gabriele Facciolo. All rights reserved.</div> )"
      ]
    }
  ]
}