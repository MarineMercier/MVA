{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Altegrad_2021_Lab2_NMT_Handout_Mercier_Marine.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PgkVw6lVUIT3",
        "pf0rN4RPToom"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCJvlnvsKALE"
      },
      "source": [
        "<center><h2>ALTeGraD 2021<br>Lab Session 2: NMT</h2><h3> Neural Machine Translation</h3> 16 / 11 / 2021<br> M. Kamal Eddine, H. Abdine</center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB6pvLvlKbtD"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqIFlSfYTwk8"
      },
      "source": [
        "## Define the Encoder / Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc8cQTFkKmif"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    to be passed the entire source sequence at once\n",
        "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
        "    https://pytorch.org/docs/stable/nn.html#gru\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n",
        "        # you should return a tensor of shape (seq, batch, feat)\n",
        "        hs, h_n = self.rnn(self.embedding(input))\n",
        "        return hs"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNnGEa5cT9ka"
      },
      "source": [
        "## Define the Decoder layer / Task 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7tLaq4PK90q"
      },
      "source": [
        "# import torch.nn.functionnal as F\n",
        "class Decoder(nn.Module):\n",
        "    '''to be used one timestep at a time\n",
        "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim_s, hidden_dim, padding_idx):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim, hidden_dim)\n",
        "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, input, source_context, h):\n",
        "        # fill the gaps #\n",
        "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
        "        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n",
        "        hs, h = self.rnn(self.embedding(input), h)\n",
        "        tilde_h = torch.tanh( self.ff_concat(torch.cat([source_context, h], dim=-1)) )\n",
        "        prediction =  self.predict(tilde_h)\n",
        "        return prediction, h"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn9iO9wNT2p7"
      },
      "source": [
        "## Define the Attention layer / Task 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwUAUDL4KmoM"
      },
      "source": [
        "class seq2seqAtt(nn.Module):\n",
        "    '''\n",
        "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
        "    https://arxiv.org/pdf/1508.04025.pdf\n",
        "    '''\n",
        "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
        "        super(seq2seqAtt, self).__init__()\n",
        "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n",
        "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n",
        "    \n",
        "    def forward(self, target_h, source_hs):\n",
        "        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n",
        "        # fill the gaps #\n",
        "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
        "        concat_output = self.ff_concat(torch.cat([target_h_rep, source_hs], dim=-1))\n",
        "        scores = self.ff_score(concat_output) # should be of shape (seq, batch, 1)\n",
        "        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
        "        norm_scores = torch.softmax(scores, dim=0)\n",
        "        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n",
        "        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
        "        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes \n",
        "        return ct, norm_scores"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUT6D3JETX8H"
      },
      "source": [
        "# Define the full seq2seq model / Task 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYX0K3dNK-c9"
      },
      "source": [
        "class seq2seqModel(nn.Module):\n",
        "    '''the full seq2seq model'''\n",
        "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
        "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
        "     'oov_token','sos_token','eos_token','max_size']\n",
        "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t, \n",
        "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
        "                 oov_token, sos_token, eos_token, max_size):\n",
        "        super(seq2seqModel, self).__init__()\n",
        "        self.vocab_s = vocab_s\n",
        "        self.source_language = source_language\n",
        "        self.vocab_t_inv = vocab_t_inv\n",
        "        self.embedding_dim_s = embedding_dim_s\n",
        "        self.embedding_dim_t = embedding_dim_t\n",
        "        self.hidden_dim_s = hidden_dim_s\n",
        "        self.hidden_dim_t = hidden_dim_t\n",
        "        self.hidden_dim_att = hidden_dim_att\n",
        "        self.do_att = do_att # should attention be used?\n",
        "        self.padding_token = padding_token\n",
        "        self.oov_token = oov_token\n",
        "        self.sos_token = sos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.max_size = max_size\n",
        "        \n",
        "        self.max_source_idx = max(list(vocab_s.values()))\n",
        "        print('max source index',self.max_source_idx)\n",
        "        print('source vocab size',len(vocab_s))\n",
        "        \n",
        "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
        "        print('max target index',self.max_target_idx)\n",
        "        print('target vocab size',len(vocab_t_inv))\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n",
        "        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_s, self.hidden_dim_t, self.padding_token).to(self.device)\n",
        "        \n",
        "        if self.do_att:\n",
        "            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n",
        "    \n",
        "    def my_pad(self, my_list):\n",
        "        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
        "        the <eos> token is appended to each sequence before padding\n",
        "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
        "        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
        "        return batch_source, batch_target\n",
        "    \n",
        "    def forward(self, input, max_size, is_prod):\n",
        "        if is_prod: \n",
        "            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
        "        current_batch_size = input.size(1)\n",
        "        # fill the gap #\n",
        "        # use the encoder\n",
        "        source_hs = self.encoder(input)\n",
        "        # = = = decoder part (one timestep at a time)  = = =\n",
        "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n",
        "        \n",
        "        # fill the gap #\n",
        "        # (initialize target_input with the proper token)\n",
        "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n",
        "        pos = 0\n",
        "        eos_counter = 0\n",
        "        logits = []\n",
        "        alignements = []\n",
        "\n",
        "        while True:\n",
        "            if self.do_att:\n",
        "                source_context, norm_scores = self.att_mech(target_h, source_hs) # (1, batch, feat)\n",
        "            else:\n",
        "                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n",
        "            # fill the gap #\n",
        "            # use the decoder\n",
        "            prediction, target_h = self.decoder(target_input, source_context, target_h)\n",
        "            logits.append(prediction) # (1, batch, vocab)\n",
        "            if is_prod:\n",
        "              alignements.append(norm_scores.detach())\n",
        "            # fill the gap #\n",
        "            # get the next input to pass the decoder\n",
        "            target_input = prediction.argmax(-1)\n",
        "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
        "            pos += 1\n",
        "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
        "                break\n",
        "        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n",
        "        \n",
        "        if is_prod:\n",
        "            to_return = to_return.squeeze(dim=1), torch.cat(alignements, 1) # (seq, vocab)\n",
        "        \n",
        "        return to_return\n",
        "    \n",
        "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
        "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
        "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "        # we pass a collate function to perform padding on the fly, within each batch\n",
        "        # this is better than truncation/padding at the dataset level\n",
        "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size, \n",
        "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n",
        "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
        "                                      collate_fn=self.my_pad)\n",
        "        tdqm_dict_keys = ['loss', 'test loss']\n",
        "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
        "        patience_counter = 1\n",
        "        patience_loss = 99999\n",
        "        \n",
        "        for epoch in range(n_epochs): \n",
        "            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n",
        "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n",
        "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
        "                    total_loss = 0\n",
        "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "                    if loader_idx == 0:\n",
        "                        self.train()\n",
        "                    else:\n",
        "                        self.eval()\n",
        "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
        "                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)                        \n",
        "                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n",
        "                        \n",
        "                        # are we using the model in production\n",
        "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n",
        "                        if is_prod:\n",
        "                            max_size = self.max_size\n",
        "                            self.eval()\n",
        "                        else:\n",
        "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
        "                        \n",
        "                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)\n",
        "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n",
        "                        total_loss += sentence_loss.item()                        \n",
        "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)                       \n",
        "                        pbar.set_postfix(tdqm_dict)                     \n",
        "                        if loader_idx == 0:\n",
        "                            optimizer.zero_grad() # flush gradient attributes\n",
        "                            sentence_loss.backward() # compute gradients\n",
        "                            optimizer.step() # update\n",
        "                            pbar.update(1)\n",
        "            \n",
        "            if total_loss > patience_loss:\n",
        "                patience_counter += 1\n",
        "            else:\n",
        "                patience_loss = total_loss\n",
        "                patience_counter = 1 # reset\n",
        "            \n",
        "            if patience_counter > patience:\n",
        "                break\n",
        "    \n",
        "    def sourceNl_to_ints(self, source_nl):\n",
        "        '''converts natural language source sentence into source integers'''\n",
        "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
        "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
        "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
        "                       self.oov_token for elt in source_nl_clean_tok]\n",
        "        \n",
        "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
        "        return source_ints \n",
        "    \n",
        "    def targetInts_to_nl(self, target_ints):\n",
        "        '''converts integer target sentence into target natural language'''\n",
        "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
        "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
        "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
        "    \n",
        "    def predict(self, source_nl):\n",
        "        source_ints = self.sourceNl_to_ints(source_nl)\n",
        "        logits, alignements = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n",
        "        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
        "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
        "        return ' '.join(target_nl), alignements\n",
        "        \n",
        "    def save(self, path_to_file):\n",
        "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
        "        attrs['state_dict'] = self.state_dict()\n",
        "        torch.save(attrs, path_to_file)\n",
        "    \n",
        "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
        "    def load(cls, path_to_file):\n",
        "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
        "        state_dict = attrs.pop('state_dict')\n",
        "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
        "        new.load_state_dict(state_dict)\n",
        "        return new        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgkVw6lVUIT3"
      },
      "source": [
        "# Prepare the Data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "datl5SFtJ9Br",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "154f5302-2048-41db-e037-f6199bf69e54"
      },
      "source": [
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\" -O \"data.zip\"\n",
        "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\" -O \"pretrained_moodle.pt\"\n",
        "!unzip data.zip\n",
        "\n",
        "path_to_data = './'\n",
        "path_to_save_models = './'\n",
        "\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "from torch.utils import data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-23 00:54:02--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://vgtdqw.am.files.1drv.com/y4mXCcmKpexAE2Z8Bvyp8skpuN6w9b6R21dMmuMb4zS7bzGBn1N0kFbbHWXGBMcLujw5No7H9mtDl5TUNpfhwKeTyw11V54mOflrbaFaTYUOnaYreMTfOM7MAxCS7TPvqocLaNalWGp6bzxrm7eRfNKepQmqf9Uj1fjDS-UlVqJqd2mgveq-itkgqA_jWe_YSiaGQX92pwf3uLZlcuqOwKTSQ/data.zip?download&psid=1 [following]\n",
            "--2021-11-23 00:54:03--  https://vgtdqw.am.files.1drv.com/y4mXCcmKpexAE2Z8Bvyp8skpuN6w9b6R21dMmuMb4zS7bzGBn1N0kFbbHWXGBMcLujw5No7H9mtDl5TUNpfhwKeTyw11V54mOflrbaFaTYUOnaYreMTfOM7MAxCS7TPvqocLaNalWGp6bzxrm7eRfNKepQmqf9Uj1fjDS-UlVqJqd2mgveq-itkgqA_jWe_YSiaGQX92pwf3uLZlcuqOwKTSQ/data.zip?download&psid=1\n",
            "Resolving vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8994805 (8.6M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   8.58M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-11-23 00:54:04 (147 MB/s) - ‘data.zip’ saved [8994805/8994805]\n",
            "\n",
            "--2021-11-23 00:54:04--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://vgtcqw.am.files.1drv.com/y4meZwk_NsCt0g9rGkqn_tg6DdYF-amrk7ZnPDq3ngIJInuoKEZC8QMAjsI02XKH27SRCXowAsbsDZzKvE7mBRGuUBfa45QQZv2aIOu3_YaIPFjxatutCN9DcoHe6eLVG0Gfn6FoxSIWueGhliWZFmxemAEknZ7L1-SRtaK8smFnQI8Pc41veOFg0LmfEWdVDPLqiX9tTHK6xNCDr-xMIUNTA/pretrained_moodle.pt?download&psid=1 [following]\n",
            "--2021-11-23 00:54:05--  https://vgtcqw.am.files.1drv.com/y4meZwk_NsCt0g9rGkqn_tg6DdYF-amrk7ZnPDq3ngIJInuoKEZC8QMAjsI02XKH27SRCXowAsbsDZzKvE7mBRGuUBfa45QQZv2aIOu3_YaIPFjxatutCN9DcoHe6eLVG0Gfn6FoxSIWueGhliWZFmxemAEknZ7L1-SRtaK8smFnQI8Pc41veOFg0LmfEWdVDPLqiX9tTHK6xNCDr-xMIUNTA/pretrained_moodle.pt?download&psid=1\n",
            "Resolving vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)... 13.107.42.12\n",
            "Connecting to vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3284775 (3.1M) [application/octet-stream]\n",
            "Saving to: ‘pretrained_moodle.pt’\n",
            "\n",
            "pretrained_moodle.p 100%[===================>]   3.13M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2021-11-23 00:54:06 (77.9 MB/s) - ‘pretrained_moodle.pt’ saved [3284775/3284775]\n",
            "\n",
            "Archive:  data.zip\n",
            " extracting: pairs_test_ints.txt     \n",
            " extracting: pairs_train_ints.txt    \n",
            " extracting: README.txt              \n",
            " extracting: vocab_source.json       \n",
            " extracting: vocab_target.json       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZCiFl61LPQj"
      },
      "source": [
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.pairs) # total nb of observations\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        source, target = self.pairs[idx] # one observation\n",
        "        return torch.LongTensor(source), torch.LongTensor(target)\n",
        "\n",
        "def load_pairs(train_or_test):\n",
        "    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n",
        "        pairs_tmp = file.read().splitlines()\n",
        "    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n",
        "    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n",
        "                  elt[1].split()]] for elt in pairs_tmp]\n",
        "    return pairs_tmp"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCsAk4ILTkEc"
      },
      "source": [
        "# Training / Task 5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSZ-cvSuLQVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752593fd-97d9-4a4b-8972-66dc482c295c"
      },
      "source": [
        "do_att = True # should always be set to True\n",
        "is_prod = False # production mode or not\n",
        "\n",
        "if not is_prod:\n",
        "        \n",
        "    pairs_train = load_pairs('train')\n",
        "    pairs_test = load_pairs('test')\n",
        "    \n",
        "    with open(path_to_data + 'vocab_source.json','r') as file:\n",
        "        vocab_source = json.load(file) # word -> index\n",
        "    \n",
        "    with open(path_to_data + 'vocab_target.json','r') as file:\n",
        "        vocab_target = json.load(file) # word -> index\n",
        "    \n",
        "    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n",
        "    \n",
        "    print('data loaded')\n",
        "        \n",
        "    training_set = Dataset(pairs_train)\n",
        "    test_set = Dataset(pairs_test)\n",
        "    \n",
        "    print('data prepared')\n",
        "    \n",
        "    print('= = = attention-based model?:',str(do_att),'= = =')\n",
        "    \n",
        "    model = seq2seqModel(vocab_s=vocab_source,\n",
        "                         source_language='english',\n",
        "                         vocab_t_inv=vocab_target_inv,\n",
        "                         embedding_dim_s=40,\n",
        "                         embedding_dim_t=41,\n",
        "                         hidden_dim_s=30,\n",
        "                         hidden_dim_t=31,\n",
        "                         hidden_dim_att=20,\n",
        "                         do_att=do_att,\n",
        "                         padding_token=0,\n",
        "                         oov_token=1,\n",
        "                         sos_token=2,\n",
        "                         eos_token=3,\n",
        "                         max_size=30) # max size of generated sentence in prediction mode\n",
        "    \n",
        "    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n",
        "    model.save(path_to_save_models + 'my_model.pt')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data loaded\n",
            "data prepared\n",
            "= = = attention-based model?: True = = =\n",
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch : 0/19: 100%|██████████████████| 2.13k/2.13k [01:30<00:00, 23.7it/s, loss=5.16, test loss=4.6]\n",
            "Epoch : 1/19: 100%|█████████████████| 2.13k/2.13k [01:30<00:00, 23.6it/s, loss=4.34, test loss=4.09]\n",
            "Epoch : 2/19: 100%|█████████████████| 2.13k/2.13k [01:30<00:00, 23.6it/s, loss=3.91, test loss=3.77]\n",
            "Epoch : 3/19: 100%|█████████████████| 2.13k/2.13k [01:30<00:00, 23.5it/s, loss=3.65, test loss=3.56]\n",
            "Epoch : 4/19: 100%|█████████████████| 2.13k/2.13k [01:30<00:00, 23.6it/s, loss=3.46, test loss=3.43]\n",
            "Epoch : 5/19: 100%|█████████████████| 2.13k/2.13k [01:31<00:00, 23.4it/s, loss=3.32, test loss=3.31]\n",
            "Epoch : 6/19: 100%|█████████████████| 2.13k/2.13k [01:31<00:00, 23.4it/s, loss=3.21, test loss=3.22]\n",
            "Epoch : 7/19: 100%|█████████████████| 2.13k/2.13k [01:30<00:00, 23.7it/s, loss=3.12, test loss=3.15]\n",
            "Epoch : 8/19: 100%|█████████████████| 2.13k/2.13k [01:30<00:00, 23.5it/s, loss=3.04, test loss=3.08]\n",
            "Epoch : 9/19: 100%|█████████████████| 2.13k/2.13k [01:30<00:00, 23.6it/s, loss=2.97, test loss=3.03]\n",
            "Epoch : 10/19: 100%|███████████████████| 2.13k/2.13k [01:30<00:00, 23.5it/s, loss=2.91, test loss=3]\n",
            "Epoch : 11/19: 100%|████████████████| 2.13k/2.13k [01:30<00:00, 23.6it/s, loss=2.86, test loss=2.96]\n",
            "Epoch : 12/19: 100%|████████████████| 2.13k/2.13k [01:30<00:00, 23.5it/s, loss=2.82, test loss=2.93]\n",
            "Epoch : 13/19: 100%|████████████████| 2.13k/2.13k [01:26<00:00, 24.6it/s, loss=2.78, test loss=2.88]\n",
            "Epoch : 14/19: 100%|████████████████| 2.13k/2.13k [01:26<00:00, 24.6it/s, loss=2.74, test loss=2.87]\n",
            "Epoch : 15/19: 100%|████████████████| 2.13k/2.13k [01:27<00:00, 24.4it/s, loss=2.71, test loss=2.83]\n",
            "Epoch : 16/19: 100%|████████████████| 2.13k/2.13k [01:26<00:00, 24.8it/s, loss=2.68, test loss=2.81]\n",
            "Epoch : 17/19: 100%|████████████████| 2.13k/2.13k [01:26<00:00, 24.6it/s, loss=2.65, test loss=2.79]\n",
            "Epoch : 18/19: 100%|████████████████| 2.13k/2.13k [01:27<00:00, 24.5it/s, loss=2.63, test loss=2.82]\n",
            "Epoch : 19/19: 100%|█████████████████| 2.13k/2.13k [01:26<00:00, 24.7it/s, loss=2.6, test loss=2.75]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf0rN4RPToom"
      },
      "source": [
        "# Testing / Task 6:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMN8E8vZxexV",
        "outputId": "4fd3d233-2b16-4a4d-a76d-b8502c5a4daa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhXbQjP_YrgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7cc867-3ece-40f2-80f2-c13242a9a30f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load(path_to_save_models + 'my_model.pt')\n",
        "    # model = seq2seqModel.load(path_to_save_models + 'my_model.pt')\n",
        "    \n",
        "    to_test = ['I am a student.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fell asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you.', # translation of mean in context\n",
        "               'She is so mean.',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed.\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek.',\n",
        "               'The cat fell asleep in front of the fireplace.']\n",
        "    \n",
        "    for elt in to_test:\n",
        "        translation, alignements = model.predict(elt)\n",
        "        print('= = = = = \\n','%s -> %s' % (elt, translation))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "= = = = = \n",
            " I am a student. -> je suis étudiant étudiant . . . . . . . . . . étudiant . j j j j j j j j j j j j j j\n",
            "= = = = = \n",
            " I have a red car. -> j ai un rouge voiture . . . . . . . . . . . . . . malchanceux malchanceux j j j j j j j j j\n",
            "= = = = = \n",
            " I love playing video games. -> j adore jouer jouer jouer jouer jeux . . . . . . . . . . . . . jeux jeux vidéo vidéo vidéo vidéo vidéo vidéo vidéo vidéo\n",
            "= = = = = \n",
            " This river is full of fish. -> cette rivière est plein de poisson poisson . . . . . . . . poisson poisson poisson poisson poisson poisson poisson poisson poisson poisson poisson poisson poisson poisson poisson\n",
            "= = = = = \n",
            " The fridge is full of food. -> le frigo est plein de nourriture nourriture . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The cat fell asleep on the mat. -> le chat est <OOV> sur la tapis . . . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " my brother likes pizza. -> mon frère aime aime la . . . . . . . . . . . . . . mon mon mon mon mon mon mon mon mon mon mon\n",
            "= = = = = \n",
            " I did not mean to hurt you. -> je n ai pas pas à me faire vous . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " She is so mean. -> elle est tellement fatigué . . . . . . . . . . . . . mort mort erreur erreur erreur erreur erreur erreur erreur erreur erreur mariée mariée\n",
            "= = = = = \n",
            " Help me pick out a tie to go with this suit! -> aidez moi à un cravate cravate à à avec ceci . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " I can't help but smoking weed. -> je ne peux pas aider aider à fumer fumer . . . . . . . . . . . . . . . . . . . . .\n",
            "= = = = = \n",
            " The kids were playing hide and seek. -> les enfants étaient que de . . . . . . . . . . . . . . . . . . . enfants enfants enfants enfants enfants enfants\n",
            "= = = = = \n",
            " The cat fell asleep in front of the fireplace. -> le chat est <OOV> devant devant . . . . . . . . . . . . . . . . . . . . . . . .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huleqmWAgqaS",
        "outputId": "112c2fd7-299d-443a-c5d0-bdf0e7c380f2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    #model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
        "    model = seq2seqModel.load(path_to_save_models + 'my_model.pt')\n",
        "    \n",
        "    to_test = ['I am a student.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fell asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you.', # translation of mean in context\n",
        "               'She is so mean.',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed.\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek.',\n",
        "               'The cat fell asleep in front of the fireplace.']\n",
        "    \n",
        "    for elt in to_test:\n",
        "        translation, alignements = model.predict(elt)\n",
        "        print('= = = = = \\n','%s -> %s' % (elt, translation))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n",
            "= = = = = \n",
            " I am a student. -> je suis une . . <EOS>\n",
            "= = = = = \n",
            " I have a red car. -> j ai une voiture voiture . <EOS>\n",
            "= = = = = \n",
            " I love playing video games. -> j adore jouer jouer jouer jouer jouer . <EOS>\n",
            "= = = = = \n",
            " This river is full of fish. -> cette poisson est est la la pièce . . <EOS>\n",
            "= = = = = \n",
            " The fridge is full of food. -> la nourriture est est la . . <EOS>\n",
            "= = = = = \n",
            " The cat fell asleep on the mat. -> le chat <OOV> <OOV> la la la . . <EOS>\n",
            "= = = = = \n",
            " my brother likes pizza. -> ma sœur aime mon . . <EOS>\n",
            "= = = = = \n",
            " I did not mean to hurt you. -> je n ai pas vraiment que que que vous vraiment . . <EOS>\n",
            "= = = = = \n",
            " She is so mean. -> elle est être . . <EOS>\n",
            "= = = = = \n",
            " Help me pick out a tie to go with this suit! -> laisse moi demander à à à à à à à . <EOS>\n",
            "= = = = = \n",
            " I can't help but smoking weed. -> je ne peux pas pas de de aider de aider . . <EOS>\n",
            "= = = = = \n",
            " The kids were playing hide and seek. -> les les sont sont les et <OOV> . . . <EOS>\n",
            "= = = = = \n",
            " The cat fell asleep in front of the fireplace. -> le chat est <OOV> la la la la . . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUmsI29nfkFZ"
      },
      "source": [
        "# Code for question 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MUXVHolcRg5"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "def plot_alignement_vector(vector, input, output,\n",
        "                          normalize=False,\n",
        "                          cmap='Greys_r'):\n",
        "\n",
        "    if normalize:\n",
        "        vector = vector.astype('float') / vector.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    input_list = input.split(\" \")\n",
        "    output_list = output.split(\" \")\n",
        "    same = 0 \n",
        "    previous = output_list[0]\n",
        "    i = 1\n",
        "    while same < 5 or i < len(output_list):\n",
        "        if output_list[i]==previous:\n",
        "            same += 1\n",
        "        else :\n",
        "            previous = output_list[i]\n",
        "            same = 0\n",
        "        i += 1\n",
        "    output_list = output_list[:i-same]\n",
        "    vector = vector[:,:i-same]\n",
        "    \n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(vector, cmap=cmap)\n",
        "    plt.title(\"Source/Target Alignments\")\n",
        "    plt.colorbar()\n",
        "    tick_marks_y = np.arange(len(input_list))\n",
        "    tick_marks_x = np.arange(len(output_list))\n",
        "    plt.xticks(tick_marks_x, output_list)\n",
        "    plt.yticks(tick_marks_y, input_list)\n",
        "\n",
        "    plt.ylabel('English sentence')\n",
        "    plt.xlabel('French translation')\n",
        "    plt.show()\n",
        "    plt.tight_layout()\n",
        "    "
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdlA3AwEbrZt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "outputId": "4405abb5-b3d1-47b2-f7d1-a04798d891b9"
      },
      "source": [
        "is_prod = True # production mode or not\n",
        "\n",
        "if is_prod:\n",
        "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
        "    \n",
        "    to_test = ['I am a student.',\n",
        "               'I have a red car.',  # inversion captured\n",
        "               'I love playing video games.',\n",
        "                'This river is full of fish.', # plein vs pleine (accord)\n",
        "                'The fridge is full of food.',\n",
        "               'The cat fell asleep on the mat.',\n",
        "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
        "               'I did not mean to hurt you', # translation of mean in context\n",
        "               'She is so mean',\n",
        "               'Help me pick out a tie to go with this suit!', # right translation\n",
        "               \"I can't help but smoking weed\", # this one and below: hallucination\n",
        "               'The kids were playing hide and seek',\n",
        "               'The cat fell asleep in front of the fireplace']\n",
        "\n",
        "    elt = to_test[1]\n",
        "    output, alignement = model.predict(elt)\n",
        "    plot_alignement_vector(alignement.cpu().numpy(), elt, output)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max source index 5281\n",
            "source vocab size 5278\n",
            "max target index 7459\n",
            "target vocab size 7456\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iN72DVlfnTZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}